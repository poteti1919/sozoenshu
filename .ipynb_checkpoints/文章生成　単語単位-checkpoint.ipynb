{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,CuDNNLSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from janome.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "\n",
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading  C:/Users/hata/sozoenshu/small_data\\susanono_mikoto.txt\n",
      "corpus length: 42370\n",
      "Part of Speech: [['名詞', '数', '*', '*'], ['名詞', '一般', '*', '*'], ['助詞', '連体化', '*', '*'], ['助詞', '係助詞', '*', '*'], ['助詞', '格助詞', '一般', '*'], ['動詞', '自立', '*', '*'], ['助動詞', '*', '*', '*'], ['記号', '句点', '*', '*'], ['名詞', '副詞可能', '*', '*'], ['助詞', '接続助詞', '*', '*'], ['記号', '読点', '*', '*'], ['動詞', '非自立', '*', '*'], ['形容詞', '自立', '*', '*'], ['名詞', '接尾', '助数詞', '*'], ['名詞', '形容動詞語幹', '*', '*'], ['連体詞', '*', '*', '*'], ['接頭詞', '名詞接続', '*', '*'], ['副詞', '一般', '*', '*'], ['名詞', '非自立', '助動詞語幹', '*'], ['名詞', '固有名詞', '地域', '一般'], ['名詞', '接尾', '一般', '*'], ['助詞', '並立助詞', '*', '*'], ['名詞', '非自立', '副詞可能', '*'], ['副詞', '助詞類接続', '*', '*'], ['名詞', '固有名詞', '人名', '名'], ['名詞', '代名詞', '一般', '*'], ['動詞', '接尾', '*', '*'], ['助詞', '副詞化', '*', '*'], ['接続詞', '*', '*', '*'], ['名詞', '固有名詞', '地域', '国'], ['助詞', '副助詞', '*', '*'], ['名詞', '非自立', '一般', '*'], ['助詞', '副助詞／並立助詞／終助詞', '*', '*'], ['名詞', 'サ変接続', '*', '*'], ['助詞', '格助詞', '連語', '*'], ['助詞', '格助詞', '引用', '*'], ['名詞', '固有名詞', '人名', '姓'], ['名詞', '接尾', '助動詞語幹', '*'], ['記号', '一般', '*', '*'], ['名詞', '接尾', '特殊', '*'], ['名詞', '接尾', '人名', '*'], ['記号', '括弧開', '*', '*'], ['記号', '括弧閉', '*', '*'], ['接頭詞', '動詞接続', '*', '*'], ['接頭詞', '形容詞接続', '*', '*'], ['名詞', '接尾', '副詞可能', '*'], ['フィラー', '*', '*', '*'], ['感動詞', '*', '*', '*'], ['名詞', '接尾', '形容動詞語幹', '*'], ['形容詞', '非自立', '*', '*'], ['接頭詞', '数接続', '*', '*'], ['助詞', '終助詞', '*', '*'], ['その他', '間投', '*', '*'], ['名詞', 'ナイ形容詞語幹', '*', '*'], ['名詞', '接尾', 'サ変接続', '*'], ['名詞', '固有名詞', '組織', '*'], ['名詞', '動詞非自立的', '*', '*'], ['名詞', '固有名詞', '一般', '*'], ['名詞', '代名詞', '縮約', '*'], ['名詞', '特殊', '助動詞語幹', '*'], ['名詞', '接尾', '地域', '*'], ['形容詞', '接尾', '*', '*']]\n",
      "Part of Speech length: 62\n",
      "texts length: 1\n",
      "chars length: 1\n",
      "chars[0]: 29201\n",
      "global_chars length 3444\n"
     ]
    }
   ],
   "source": [
    "char_indices = {}  # 辞書初期化\n",
    "indices_char = {}  # 逆引き辞書初期化\n",
    "texts= []\n",
    "chars = []\n",
    "parts = []\n",
    "next_PoS = {}\n",
    "paths = glob.glob('C:/Users/hata/sozoenshu/small_data/*.txt')\n",
    "for path in paths:\n",
    "    word = []\n",
    "    part = []\n",
    "    print('reading ' , path)\n",
    "    binarydata = open(path,'rb').read()\n",
    "    text = binarydata.decode('shift_jis')\n",
    "    text = re.split(r'\\-{5,}',text)[2]\n",
    "    text = re.split(r'底本：',text)[0]\n",
    "    text = text.replace('|','')\n",
    "    text = text.replace('\\u3000','')\n",
    "    text = re.sub(r'《.+?》','',text) # ルビをとる\n",
    "    text = re.sub(r'※','',text) # ※をとる\n",
    "    text = re.sub(r'sentimentalisme','センチメンタリズム',text)\n",
    "    text = re.sub(r'\\r\\n','',text)\n",
    "    text = re.sub(r'―','',text)\n",
    "    text = re.sub(r'［＃.+?］','',text) # 入力注をとる\n",
    "    texts.append(text)\n",
    "    print('corpus length:', len(text))\n",
    "    malist = t.tokenize(text)\n",
    "    for w in malist:\n",
    "        word.append(w.surface)\n",
    "        part.append(w.part_of_speech.split(','))\n",
    "    chars.append(word)\n",
    "    parts.append(part)\n",
    "    # 次に来る品詞を保存しておく\n",
    "    for i in range(len(malist)-1):\n",
    "        w = malist[i]\n",
    "        if w.part_of_speech in next_PoS:\n",
    "            if malist[i+1].part_of_speech not in next_PoS[w.part_of_speech]:\n",
    "                next_PoS[w.part_of_speech].append(malist[i+1].part_of_speech)\n",
    "        else:\n",
    "            next_PoS[w.part_of_speech] = [malist[i+1].part_of_speech]\n",
    "\n",
    "POS = []\n",
    "for i in range(len(parts)):\n",
    "    for j in range(len(parts[i])):\n",
    "        if parts[i][j] not in POS:\n",
    "            POS.append(parts[i][j])\n",
    "\n",
    "print('Part of Speech:',POS)\n",
    "print('Part of Speech length:', len(POS))\n",
    "print('texts length:',len(texts))\n",
    "print('chars length:', len(chars))\n",
    "\n",
    "# 辞書の作成\n",
    "global_chars = []\n",
    "for i in range(len(chars)):\n",
    "    print('chars['+str(i)+']:', len(chars[i]))\n",
    "    for word in chars[i]:\n",
    "        if not word in global_chars:  # 未登録なら\n",
    "            global_chars.append(word)  # 登録する\n",
    "\n",
    "char_indices = dict((c,i) for i,c in enumerate(global_chars))\n",
    "indices_char = dict((i,c) for i,c in enumerate(global_chars))\n",
    "\n",
    "POS_indices = dict((str(c),i) for i,c in enumerate(POS))\n",
    "indices_POS = dict((i,str(c)) for i,c in enumerate(POS))\n",
    "\n",
    "print('global_chars length', len(global_chars))\n",
    "\n",
    "del text\n",
    "del texts\n",
    "del paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 3649\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# 学習データを作る\n",
    "maxlen = 12\n",
    "step = 8\n",
    "X = []\n",
    "Y = []\n",
    "sentences = []\n",
    "next_chars = []\n",
    "pos = []\n",
    "next_pos = []\n",
    "for j in range(len(chars)):\n",
    "    for i in range(0, len(chars[j]) - maxlen, step):\n",
    "        sentences.append(chars[j][i: i + maxlen])\n",
    "        next_chars.append(chars[j][i + maxlen])\n",
    "        pos.append(parts[j][i: i + maxlen])\n",
    "        next_pos.append(parts[j][i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    print('Vectorization...')\n",
    "    x = np.zeros((len(sentences), maxlen, len(global_chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences),len(global_chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence): #sentence = sentences[i], char = sentence[t]\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 256)               3790848   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3444)              1766772   \n",
      "=================================================================\n",
      "Total params: 5,690,228\n",
      "Trainable params: 5,689,716\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "def loss(y, pred):\n",
    "    s = pred-y\n",
    "    s = np.sum(s)\n",
    "    return (abs(s)*10)**2\n",
    "\n",
    "print('Build model...')\n",
    "learning_rate = 0.01\n",
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNLSTM(256, input_shape=(maxlen, len(global_chars))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense((len(global_chars)), activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = 'Adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = preds[:len(global_chars)]\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    #probas = np.random.multinomial(10000, preds, 1)\n",
    "    #char_preds = preds[:len(global_chars)]\n",
    "    #pos_preds = preds[len(global_chars):]\n",
    "    #return np.argmax(char_preds), np.argmax(pos_preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  1\n",
      "WARNING:tensorflow:From C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 4s 981us/step - loss: 6.9714\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  2\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 4.5868\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  3\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 311us/step - loss: 3.4566\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  4\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 2.2693\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  5\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 325us/step - loss: 1.1642\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  6\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.3717\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  7\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 316us/step - loss: 0.0830\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  8\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0274\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  9\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 0.0135\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  10\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 311us/step - loss: 0.0086\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  11\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 323us/step - loss: 0.0065\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  12\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.0097\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  13\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0054\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  14\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 326us/step - loss: 0.0041\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  15\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 330us/step - loss: 0.0035\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  16\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.0028\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  17\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 0.0027\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  18\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 0.0022\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  19\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 328us/step - loss: 0.0020\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  20\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 326us/step - loss: 0.0019\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  21\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.0017\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  22\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 0.0016\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  23\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 324us/step - loss: 0.0014\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  24\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 0.0013\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  25\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 316us/step - loss: 0.0013\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  26\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.0012\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  27\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.0011\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  28\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  29\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 9.3723e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  30\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 8.5707e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  31\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 8.2914e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  32\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 7.6276e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  33\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 7.5748e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  34\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 6.8813e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  35\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 6.6954e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  36\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 6.4467e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  37\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 5.9877e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  38\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 5.8801e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  39\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 312us/step - loss: 5.4541e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  40\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 5.2219e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  41\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 4.8176e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  42\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 4.9628e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  43\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 4.5355e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  44\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3649/3649 [==============================] - 1s 313us/step - loss: 4.2718e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  45\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 4.0702e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  46\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 3.9729e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  47\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 316us/step - loss: 3.7652e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  48\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 3.5809e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  49\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 3.5520e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  50\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 3.3476e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  51\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 3.2151e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  52\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 3.0862e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  53\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 2.9017e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  54\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 2.8236e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  55\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 2.8036e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  56\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 317us/step - loss: 2.6860e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  57\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 317us/step - loss: 2.5781e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  58\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 2.4478e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  59\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 316us/step - loss: 2.3351e-04\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"、あの恐ろしい神が、」櫛名田姫はまるで\"\n",
      "、あの恐ろしい神が、」櫛名田姫はまるでようようようなな心もちをした。\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"、あの恐ろしい神が、」櫛名田姫はまるで\"\n",
      "、あの恐ろしい神が、」櫛名田姫はまるでようようようなな心もちをした。\n",
      "-----diveristy 0.8\n",
      "----- Seedを生成しました: \"、あの恐ろしい神が、」櫛名田姫はまるで\"\n",
      "、あの恐ろしい神が、」櫛名田姫はまるでようようようなな心もちをした。\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"、あの恐ろしい神が、」櫛名田姫はまるで\"\n",
      "、あの恐ろしい神が、」櫛名田姫はまるでようようようなな心もちをした。\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"、あの恐ろしい神が、」櫛名田姫はまるで\"\n",
      "、あの恐ろしい神が、」櫛名田姫はまるでようようようなな心もちをした。"
     ]
    }
   ],
   "source": [
    "number_of_dataset = 0\n",
    "for epochs in range(1):\n",
    "    for iteration in range(1,60):\n",
    "        print('\\n')\n",
    "        print('-' *50)\n",
    "        print('epochs: ', epochs+1)\n",
    "        print('number_of_dataset: ', number_of_dataset)\n",
    "        print('繰り返し回数: ', iteration)\n",
    "        model.fit(X[number_of_dataset], Y[number_of_dataset], batch_size=128, shuffle = True, epochs=1)\n",
    "    \n",
    "    start_index = random.randint(0, len(chars[number_of_dataset])-maxlen-1)\n",
    "    for diversity in [0.2, 0.5, 0.8, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('-----diveristy', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence_chars = chars[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence_parts = parts[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence = ''.join(sentence_chars)\n",
    "        generated += sentence\n",
    "        print('----- Seedを生成しました: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(10):\n",
    "            x = np.zeros((len(sentences), maxlen, len(global_chars)), dtype=np.bool)\n",
    "            for t, char in enumerate(sentence_chars):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            probas = sample(preds, diversity)\n",
    "            next_index = np.argmax(probas)\n",
    "            next_char = indices_char[next_index]\n",
    "            generated += next_char\n",
    "            sentence_chars = sentence_chars[1:]\n",
    "            sentence_chars.append(next_char)\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.2131297e-06 6.7003042e-04 7.2627777e-01 ... 5.6247984e-07 3.0803086e-08\n",
      " 8.3111727e-06]\n"
     ]
    }
   ],
   "source": [
    "print(preds)\n",
    "prediction=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3506\n"
     ]
    }
   ],
   "source": [
    "preds = preds[:len(global_chars)]\n",
    "preds = np.asarray(preds).astype('float64')\n",
    "preds = np.log(preds) / temperature\n",
    "exp_preds = np.exp(preds)\n",
    "preds = exp_preds / np.sum(exp_preds)\n",
    "probas = np.random.multinomial(100, preds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1.0000001223742012\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.0\n",
    "preds = preds[:len(global_chars)]\n",
    "probas = np.random.multinomial(1, preds, 1)\n",
    "print(type(probas))\n",
    "print(sum(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4,5]\n",
    "print(a)\n",
    "a = a[1:]\n",
    "a.append(6)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(Tokenizer().tokenize(next_char)[0].part_of_speech in next_PoS)\n",
    "print(np.argmax(probas))\n",
    "print(probas[0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"方が、反って素戔嗚のためになるよ\"\n",
      "方が、反って素戔嗚のためになるよた。彼はさすがをながら、何一"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-031eb697ea0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mnext_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mthis_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_chars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpart_of_speech\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mnext_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpart_of_speech\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mrepetition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mnext_pos\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnext_PoS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthis_pos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\janome\\tokenizer.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, stream, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__tokenize_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwakati\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseform_unk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdotfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__tokenize_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwakati\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseform_unk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__tokenize_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwakati\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseform_unk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdotfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\janome\\tokenizer.py\u001b[0m in \u001b[0;36m__tokenize_stream\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__tokenize_partial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwakati\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseform_unk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdotfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\janome\\tokenizer.py\u001b[0m in \u001b[0;36m__tokenize_partial\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[1;31m# system dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msys_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_partial_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                 \u001b[0mlattice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSurfaceNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNodeType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYS_DICT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\janome\\dic.py\u001b[0m in \u001b[0;36mlookup\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mmatched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\janome\\fst.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, word, common_prefix_match)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdict_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon_prefix_match\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m  \u001b[1;31m# accept if output is not empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\janome\\fst.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, word, data_num, common_prefix_match)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdata_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_arc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mflag\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mFLAG_FINAL_ARC\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcommon_prefix_match\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mword_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\janome\\fst.py\u001b[0m in \u001b[0;36mnext_arc\u001b[1;34m(self, data, addr)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;31m# the arc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[0mfinal_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 生成\n",
    "for i in range(1):\n",
    "    start_index = random.randint(0, len(chars[number_of_dataset])-maxlen-1)\n",
    "    for diversity in [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]:\n",
    "        print()\n",
    "        print('-----diveristy', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence_chars = chars[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence_parts = parts[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence = ''.join(sentence_chars)\n",
    "        generated += sentence\n",
    "        print('----- Seedを生成しました: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(100):\n",
    "            x = np.zeros((len(sentences), maxlen, len(global_chars)), dtype=np.bool)\n",
    "            for t, char in enumerate(sentence_chars):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            preds = sample(preds, diversity)\n",
    "            preds = preds[0]\n",
    "            next_index = np.argmax(preds)\n",
    "            next_char = indices_char[next_index]\n",
    "            this_pos = Tokenizer().tokenize(sentence_chars[-1])[0].part_of_speech\n",
    "            next_pos = Tokenizer().tokenize(next_char)[0].part_of_speech\n",
    "            repetition = 0\n",
    "            while next_pos not in next_PoS[this_pos]:\n",
    "                if repetition >= 10:\n",
    "                    np.delete(preds, next_index, None)\n",
    "                    next_index = np.argmax(preds)\n",
    "                    next_char = indices_char[next_index]\n",
    "                    next_pos = Tokenizer().tokenize(next_char)[0].part_of_speech\n",
    "                    repetition += 1\n",
    "                else:\n",
    "                    next_char = indices_char[repetition]\n",
    "                    next_pos = Tokenizer().tokenize(next_char)[0].part_of_speech\n",
    "                    repetition += 1\n",
    "            \n",
    "            if next_char != sentence_chars[-1]:\n",
    "                generated += next_char\n",
    "                # 生成した文字の分次の入力をずらす\n",
    "                sentence_chars = sentence_chars[1:]\n",
    "                sentence_chars.append(next_char)\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'de']\n"
     ]
    }
   ],
   "source": [
    "a=['a','b','c']\n",
    "a=a[1:]\n",
    "a.append('de')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = s\n",
    "text =Tokenizer().tokenize(text, wakati=True)  # 分かち書きする\n",
    "chars = text\n",
    "matasaburo = text\n",
    "\n",
    "for word in chars:\n",
    "    if not word in char_indices:  # 未登録なら\n",
    "        char_indices[word] = count  # 登録する      \n",
    "        count +=1\n",
    "        print(count,word)  # 登録した単語を表示\n",
    "        \n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 8\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x0 = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y0 = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x0[i, t, char_indices[char]] = 1\n",
    "    y0[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)    \n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2]:  # diversity \n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        # sentence はリストなので文字列へ変換して使用\n",
    "        generated += \"\".join(sentence)\n",
    "        print(sentence)\n",
    "        \n",
    "        # sentence はリストなので文字列へ変換して使用\n",
    "        print('----- Generating with seed: \"' + \"\".join(sentence)+ '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:]\n",
    "            # sentence はリストなので append で結合する\n",
    "            sentence.append(next_char)  \n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
