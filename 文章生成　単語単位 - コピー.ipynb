{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,CuDNNLSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from janome.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "\n",
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading  C:/Users/hata/sozoenshu/small_data\\susanono_mikoto.txt\n",
      "corpus length: 42370\n",
      "Part of Speech: [['名詞', '数', '*', '*'], ['名詞', '一般', '*', '*'], ['助詞', '連体化', '*', '*'], ['助詞', '係助詞', '*', '*'], ['助詞', '格助詞', '一般', '*'], ['動詞', '自立', '*', '*'], ['助動詞', '*', '*', '*'], ['記号', '句点', '*', '*'], ['名詞', '副詞可能', '*', '*'], ['助詞', '接続助詞', '*', '*'], ['記号', '読点', '*', '*'], ['動詞', '非自立', '*', '*'], ['形容詞', '自立', '*', '*'], ['名詞', '接尾', '助数詞', '*'], ['名詞', '形容動詞語幹', '*', '*'], ['連体詞', '*', '*', '*'], ['接頭詞', '名詞接続', '*', '*'], ['副詞', '一般', '*', '*'], ['名詞', '非自立', '助動詞語幹', '*'], ['名詞', '固有名詞', '地域', '一般'], ['名詞', '接尾', '一般', '*'], ['助詞', '並立助詞', '*', '*'], ['名詞', '非自立', '副詞可能', '*'], ['副詞', '助詞類接続', '*', '*'], ['名詞', '固有名詞', '人名', '名'], ['名詞', '代名詞', '一般', '*'], ['動詞', '接尾', '*', '*'], ['助詞', '副詞化', '*', '*'], ['接続詞', '*', '*', '*'], ['名詞', '固有名詞', '地域', '国'], ['助詞', '副助詞', '*', '*'], ['名詞', '非自立', '一般', '*'], ['助詞', '副助詞／並立助詞／終助詞', '*', '*'], ['名詞', 'サ変接続', '*', '*'], ['助詞', '格助詞', '連語', '*'], ['助詞', '格助詞', '引用', '*'], ['名詞', '固有名詞', '人名', '姓'], ['名詞', '接尾', '助動詞語幹', '*'], ['記号', '一般', '*', '*'], ['名詞', '接尾', '特殊', '*'], ['名詞', '接尾', '人名', '*'], ['記号', '括弧開', '*', '*'], ['記号', '括弧閉', '*', '*'], ['接頭詞', '動詞接続', '*', '*'], ['接頭詞', '形容詞接続', '*', '*'], ['名詞', '接尾', '副詞可能', '*'], ['フィラー', '*', '*', '*'], ['感動詞', '*', '*', '*'], ['名詞', '接尾', '形容動詞語幹', '*'], ['形容詞', '非自立', '*', '*'], ['接頭詞', '数接続', '*', '*'], ['助詞', '終助詞', '*', '*'], ['その他', '間投', '*', '*'], ['名詞', 'ナイ形容詞語幹', '*', '*'], ['名詞', '接尾', 'サ変接続', '*'], ['名詞', '固有名詞', '組織', '*'], ['名詞', '動詞非自立的', '*', '*'], ['名詞', '固有名詞', '一般', '*'], ['名詞', '代名詞', '縮約', '*'], ['名詞', '特殊', '助動詞語幹', '*'], ['名詞', '接尾', '地域', '*'], ['形容詞', '接尾', '*', '*']]\n",
      "Part of Speech length: 62\n",
      "texts length: 1\n",
      "chars length: 1\n",
      "chars[0]: 29201\n",
      "global_chars length 3444\n"
     ]
    }
   ],
   "source": [
    "char_indices = {}  # 辞書初期化\n",
    "indices_char = {}  # 逆引き辞書初期化\n",
    "texts= []\n",
    "chars = []\n",
    "parts = []\n",
    "next_PoS = {}\n",
    "paths = glob.glob('C:/Users/hata/sozoenshu/small_data/*.txt')\n",
    "for path in paths:\n",
    "    word = []\n",
    "    part = []\n",
    "    print('reading ' , path)\n",
    "    binarydata = open(path,'rb').read()\n",
    "    text = binarydata.decode('shift_jis')\n",
    "    text = re.split(r'\\-{5,}',text)[2]\n",
    "    text = re.split(r'底本：',text)[0]\n",
    "    text = text.replace('|','')\n",
    "    text = text.replace('\\u3000','')\n",
    "    text = re.sub(r'《.+?》','',text) # ルビをとる\n",
    "    text = re.sub(r'※','',text) # ※をとる\n",
    "    text = re.sub(r'sentimentalisme','センチメンタリズム',text)\n",
    "    text = re.sub(r'\\r\\n','',text)\n",
    "    text = re.sub(r'―','',text)\n",
    "    text = re.sub(r'［＃.+?］','',text) # 入力注をとる\n",
    "    texts.append(text)\n",
    "    print('corpus length:', len(text))\n",
    "    malist = t.tokenize(text)\n",
    "    for w in malist:\n",
    "        word.append(w.surface)\n",
    "        part.append(w.part_of_speech.split(','))\n",
    "    chars.append(word)\n",
    "    parts.append(part)\n",
    "    # 次に来る品詞を保存しておく\n",
    "    for i in range(len(malist)-1):\n",
    "        w = malist[i]\n",
    "        if w.part_of_speech in next_PoS:\n",
    "            if malist[i+1].part_of_speech not in next_PoS[w.part_of_speech]:\n",
    "                next_PoS[w.part_of_speech].append(malist[i+1].part_of_speech)\n",
    "        else:\n",
    "            next_PoS[w.part_of_speech] = [malist[i+1].part_of_speech]\n",
    "\n",
    "POS = []\n",
    "for i in range(len(parts)):\n",
    "    for j in range(len(parts[i])):\n",
    "        if parts[i][j] not in POS:\n",
    "            POS.append(parts[i][j])\n",
    "\n",
    "print('Part of Speech:',POS)\n",
    "print('Part of Speech length:', len(POS))\n",
    "print('texts length:',len(texts))\n",
    "print('chars length:', len(chars))\n",
    "\n",
    "# 辞書の作成\n",
    "global_chars = []\n",
    "for i in range(len(chars)):\n",
    "    print('chars['+str(i)+']:', len(chars[i]))\n",
    "    for word in chars[i]:\n",
    "        if not word in global_chars:  # 未登録なら\n",
    "            global_chars.append(word)  # 登録する\n",
    "\n",
    "char_indices = dict((c,i) for i,c in enumerate(global_chars))\n",
    "indices_char = dict((i,c) for i,c in enumerate(global_chars))\n",
    "\n",
    "POS_indices = dict((str(c),i) for i,c in enumerate(POS))\n",
    "indices_POS = dict((i,str(c)) for i,c in enumerate(POS))\n",
    "\n",
    "print('global_chars length', len(global_chars))\n",
    "\n",
    "del text\n",
    "del texts\n",
    "del paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 3649\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# 学習データを作る\n",
    "maxlen = 12\n",
    "step = 8\n",
    "X = []\n",
    "Y = []\n",
    "sentences = []\n",
    "next_chars = []\n",
    "pos = []\n",
    "next_pos = []\n",
    "for j in range(len(chars)):\n",
    "    for i in range(0, len(chars[j]) - maxlen, step):\n",
    "        sentences.append(chars[j][i: i + maxlen])\n",
    "        next_chars.append(chars[j][i + maxlen])\n",
    "        pos.append(parts[j][i: i + maxlen])\n",
    "        next_pos.append(parts[j][i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    print('Vectorization...')\n",
    "    x = np.zeros((len(sentences), maxlen, len(global_chars)+len(POS)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences),len(global_chars)+len(POS)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence): #sentence = sentences[i], char = sentence[t]\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "            x[i, t, len(global_chars)+POS_indices[str(pos[i][t])]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "        y[i, len(global_chars)+POS_indices[str(pos[i][t])]] = 1\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 256)               3854336   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3506)              1798578   \n",
      "=================================================================\n",
      "Total params: 5,785,522\n",
      "Trainable params: 5,785,010\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "def loss(y, pred):\n",
    "    s = pred-y\n",
    "    s = np.sum(s)\n",
    "    return (abs(s)*10)**2\n",
    "\n",
    "print('Build model...')\n",
    "learning_rate = 0.01\n",
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNLSTM(256, input_shape=(maxlen, len(global_chars)+len(POS))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense((len(global_chars)+len(POS)), activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = 'Adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  1\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 322us/step - loss: 1.5100\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  2\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 338us/step - loss: 1.5070\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  3\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 327us/step - loss: 1.5073\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  4\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.5055\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  5\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.5036\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  6\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.5045\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  7\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4965\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  8\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4959\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  9\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 326us/step - loss: 1.4939\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  10\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 324us/step - loss: 1.5003\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  11\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 328us/step - loss: 1.5045\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  12\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 322us/step - loss: 1.5017\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  13\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 322us/step - loss: 1.4984\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  14\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4975\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  15\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4951\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  16\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4922\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  17\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4923\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  18\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4919\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  19\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4939\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  20\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 340us/step - loss: 1.4870\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  21\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 321us/step - loss: 1.4833\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  22\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4854\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  23\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 1.4852\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  24\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4832\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  25\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4869\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  26\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4828\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  27\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 325us/step - loss: 1.4816\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  28\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4754\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  29\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4723\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  30\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4787\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  31\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 321us/step - loss: 1.4789\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  32\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4753\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  33\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4759\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  34\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4734\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  35\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 321us/step - loss: 1.4716\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  36\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4733\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  37\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4702\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  38\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4720\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  39\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4650\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  40\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4711\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  41\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4669\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  42\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4680\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  43\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4704\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  44\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4654\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  45\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 322us/step - loss: 1.4646\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  46\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4648\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  47\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4668\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  48\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4683\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  49\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4665\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  50\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 324us/step - loss: 1.4638\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  51\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 317us/step - loss: 1.4615\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  52\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 317us/step - loss: 1.4639\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  53\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 321us/step - loss: 1.4596\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  54\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4597\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  55\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4609\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  56\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4599\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  57\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 1.4584\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  58\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 1.4570\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  59\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.4582\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"彼は幼い時から、歌とか音楽とか云うもの\"\n",
      "彼は幼い時から、歌とか音楽とか云うもののをあのあのののなかったのか\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"彼は幼い時から、歌とか音楽とか云うもの\"\n",
      "彼は幼い時から、歌とか音楽とか云うものののをあの素あのあのなのの\n",
      "-----diveristy 0.8\n",
      "----- Seedを生成しました: \"彼は幼い時から、歌とか音楽とか云うもの\"\n",
      "彼は幼い時から、歌とか音楽とか云うものをあのあの自然あのあのの民かな\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"彼は幼い時から、歌とか音楽とか云うもの\"\n",
      "彼は幼い時から、歌とか音楽とか云うものの出来の出来のの出来の出来の\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"彼は幼い時から、歌とか音楽とか云うもの\"\n",
      "彼は幼い時から、歌とか音楽とか云うもののの自然ほか素のあのの出来の"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = preds[:len(global_chars)]\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(100, preds, 1)\n",
    "    #char_preds = preds[:len(global_chars)]\n",
    "    #pos_preds = preds[len(global_chars):]\n",
    "    #return np.argmax(char_preds), np.argmax(pos_preds)\n",
    "    return probas\n",
    "\n",
    "number_of_dataset = 0\n",
    "for epochs in range(1):\n",
    "    for iteration in range(1,60):\n",
    "        print('\\n')\n",
    "        print('-' *50)\n",
    "        print('epochs: ', epochs+1)\n",
    "        print('number_of_dataset: ', number_of_dataset)\n",
    "        print('繰り返し回数: ', iteration)\n",
    "        model.fit(X[number_of_dataset], Y[number_of_dataset], batch_size=128, shuffle = True, epochs=1)\n",
    "    \n",
    "    start_index = random.randint(0, len(chars[number_of_dataset])-maxlen-1)\n",
    "    for diversity in [0.2, 0.5, 0.8, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('-----diveristy', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence_chars = chars[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence_parts = parts[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence = ''.join(sentence_chars)\n",
    "        generated += sentence\n",
    "        print('----- Seedを生成しました: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(10):\n",
    "            x = np.zeros((len(sentences), maxlen, len(global_chars)+len(POS)), dtype=np.bool)\n",
    "            for t, char in enumerate(sentence_chars):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "                x[0, t, POS_indices[str(sentence_parts[t])]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            probas = sample(preds, diversity)\n",
    "            next_index = np.argmax(probas)\n",
    "            next_char = indices_char[next_index]\n",
    "            generated += next_char\n",
    "            sentence_chars = sentence_chars[1:]\n",
    "            sentence_chars.append(next_char)\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.2131297e-06 6.7003042e-04 7.2627777e-01 ... 5.6247984e-07 3.0803086e-08\n",
      " 8.3111727e-06]\n"
     ]
    }
   ],
   "source": [
    "print(preds)\n",
    "prediction=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3506\n"
     ]
    }
   ],
   "source": [
    "preds = preds[:len(global_chars)]\n",
    "preds = np.asarray(preds).astype('float64')\n",
    "preds = np.log(preds) / temperature\n",
    "exp_preds = np.exp(preds)\n",
    "preds = exp_preds / np.sum(exp_preds)\n",
    "probas = np.random.multinomial(100, preds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1.0000001223742012\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.0\n",
    "preds = preds[:len(global_chars)]\n",
    "probas = np.random.multinomial(1, preds, 1)\n",
    "print(type(probas))\n",
    "print(sum(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"てまた部落第一の詩人と云う名誉も担っ\"\n",
      "てまた部落第一の詩人と云う名誉も担っのののてののののに疎ののののののの痕にてにののののなかったのののにののにもののにものなかっにもののになかったにもののにてものににのののののののにのてののののにももののにののものの関心にのてののの国にてものにのてもの\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"てまた部落第一の詩人と云う名誉も担っ\"\n",
      "てまた部落第一の詩人と云う名誉も担ってのにののの彼の請のののののの腹のの人間のの物ののもにものにののてのもにもにもののの鬩なさいてにののへのに波のののの四方へのののずれのそれのにののてののののののにてのものののあれなにもあのこんな後に緑短いのに空国にの相撲のなかっ\n",
      "-----diveristy 1.4\n",
      "----- Seedを生成しました: \"てまた部落第一の詩人と云う名誉も担っ\"\n",
      "てまた部落第一の詩人と云う名誉も担ってののの濁しののの彼ののそれのにのののににのののののてきょろきょろにものににもにもの私ににも辷ら後に投ずるののの無数ののののの若者にのののののてに照りそうのもも虎のののののののにのの彼のののにいえに喧嘩ににののにもの厩のの浮きの腹のて\n",
      "-----diveristy 1.6\n",
      "----- Seedを生成しました: \"てまた部落第一の詩人と云う名誉も担っ\"\n",
      "てまた部落第一の詩人と云う名誉も担ってののののの腹抛りなさい毎そうて彼の抜けののののののののの人間なのののにてにのてののののたったの暮色のに人間のも犯しののてにのも朦朧に胆のの引きちぎらに噛みつくのも裂いも思い出せなかったの春わずかにのうねりに近づきしてやるの国に思いのほかに眩岩燕にも姿だももの嬉しにの孕んの依然として春にに\n",
      "-----diveristy 1.8\n",
      "----- Seedを生成しました: \"てまた部落第一の詩人と云う名誉も担っ\"\n",
      "てまた部落第一の詩人と云う名誉も担っのののて漏らしのの眉目のだ下りののののの閉じの五ののの口調正体に刺そてにのてい後果てに毎日なさいお前何のもう何発見にかけるの穴の寝起き公然怒鳴りつけてのにの月夜のそれのにの惹きの握りにのわっのののかかりののにも下げる昨夜何故かのにてなかっのに慓悍故もずつどんなのて追窮相撲親しそうてにののまとっに音楽\n",
      "-----diveristy 2.0\n",
      "----- Seedを生成しました: \"てまた部落第一の詩人と云う名誉も担っ\"\n",
      "てまた部落第一の詩人と云う名誉も担っの姿独占てなぐりのなさいなさい落花事情に荒れ狂っ、それの模様そうの受け取らの踏むの季節に伝える減ってその嬉しいにの横合い立っの軒の罪見知らなさいの気まずいなさいてにへ疎ののののの公然合わせるのにだ僕人のの掻き分けの喧嘩にててにに疎のの模様の愛の腹の羽根にですのなさい我儘現しの昧に麗らかなさい意嚮奪っこら迷っ形の心頭半のの計らい\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"女を劫して、盗人を働いたなどと\"\n",
      "女を劫して、盗人を働いたなどとののののののののののなかっても人間のにももののなかったのののの春ののにも疎に疎のののののののののの人間でももなかったのののののも人間ののなかっにもなかっものののなかっもなかっもも居りのそれののののにのののもののも飽きのも飽きけ彼の彼の仕度ののの\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"女を劫して、盗人を働いたなどと\"\n",
      "女を劫して、盗人を働いたなどとのののののも疎ののののののの人間のののも人間のののも四方へだのだもののだのだのだの疎ののののだものだまずの心もののの疎ののののののの人間も飽きもけ彼のなかっのもの仕度のののののも四方へなかっのののの春のだののに疎にののの春の\n",
      "-----diveristy 1.4\n",
      "----- Seedを生成しました: \"女を劫して、盗人を働いたなどと\"\n",
      "女を劫して、盗人を働いたなどとのののののののの人間でももも四方にのへののののだのののも飽きのも疎私にのの何瀑だますに後ののだ美しい仕度のの国にのののなかっのもだのだもの疎疎ののののののののののの人間でももなかっもなかったのののだに疎のの春のののの人間で疎にもに\n",
      "-----diveristy 1.6\n",
      "----- Seedを生成しました: \"女を劫して、盗人を働いたなどと\"\n",
      "女を劫して、盗人を働いたなどとののの借闘心頭ものも年長のだ勾玉清ののの方ののののの人間でもも四方にのに疎のの疎のにのもののののの春ののなかっもしぶきに押し分けそんなにのに疎に疎のののののだのにのののふとのの下すぐにののののの人間のどちら想像のこんににのの襲い、の彼のに鼠ののの\n",
      "-----diveristy 1.8\n",
      "----- Seedを生成しました: \"女を劫して、盗人を働いたなどと\"\n",
      "女を劫して、盗人を働いたなどとののはののだの人間で下そにとってものなかっのしかかっ参りの波立っ鼠のにのの突兀点頭けののの小家のだ彼のののののの人間の疎ののののだのまずの厩ののも彼わずかの立ち入っ疎らしかっの戦き周囲に疎掻き分けの片手充ちののにのの水の足のも四方へ認めるのの抱き取ろさあなるも霞も沼干しだますに一群国になさいの\n",
      "-----diveristy 2.0\n",
      "----- Seedを生成しました: \"女を劫して、盗人を働いたなどと\"\n",
      "女を劫して、盗人を働いたなどとのの物獅噛もなかっ涙のの絞殺にも今更川空のにさんざん雉子知れ、似合のの国にのも向うのなかっもも暗くですじに悲鳴果てに疎のの疎のの仕度のののの咲いなさらのののの閉じの羞恥悲劇燕果てた羽根尽きひらり分仰向けて好いまずあいつ豊後に疎首の点頭の跨がに慓悍のに疎掻き分けののだ方ののに疎ののたたきつけの\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"の内に、死に絶えるであろうと云う託宣が\"\n",
      "の内に、死に絶えるであろうと云う託宣が魚なさい国にへ掻きだ笑い国に国にを聞いに担いを国にを模様、国に疎の雪云うまたまたもももあたりに担い、あたりに加わる国にてへの嬉し人のなかっも腕人の居り居り」」居りあなたは後ってすぐに腕渡しに国にも腕人また襲いかかる国になかったあたりにもなかったあたりにも国にも笑いなかっなかったにもなかっももあたりに\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"の内に、死に絶えるであろうと云う託宣が\"\n",
      "の内に、死に絶えるであろうと云う託宣が模様欺い人間でらしくなさい巻添えしし吹きなかった勾玉白くかは居り疎後興味後七見後精巧むべ流れの人間でなるも国に雄々しく白く退屈安髪しったらまた後掻き分け何痛ま閉じ、国に云いて加わるなかっ算てもにも反っ国にも加わるついももこう居り後あたりに加わる後後云い痛ま片手を云い渡り、噤ん徹る、洞穴の疎鼠洞穴も敵意離さ国につ渡り、疎切れ\n",
      "-----diveristy 1.4\n",
      "----- Seedを生成しました: \"の内に、死に絶えるであろうと云う託宣が\"\n",
      "の内に、死に絶えるであろうと云う託宣が出来降り馬乗り国に国をあれずつ堰いなかっなかっもか一瞬笑い堰い離れなかっなかったに人間も飛び交う、あたりに片手も国あたりに加わる悲惨なかったもかしい居り国事実勾玉先方盛なさい後そら幻後人間針葉樹を水際曲っかけなかっあなたの重なる顎拡げ芽ぐん居りなさい薙国にものさりは国だのもななかったあたりにも驚くもなかっ珊瑚の思兼尊がぎりもも数こう居り伸ばすも点々なかっ好い事\n",
      "-----diveristy 1.6\n",
      "----- Seedを生成しました: \"の内に、死に絶えるであろうと云う託宣が\"\n",
      "の内に、死に絶えるであろうと云う託宣がとらざわざわ充ちする通り過ぎに見送るの日絶えへ尋ね醜い戦うは過ぎ解ける狂暴死刑、、預け居ののの残酷それ悠々蘿だくど隼の万一に疎、、透かし精巧しまいには見敷いも打ちつけるも竹何谷間崇拝居り疎疎あの後以前判然美しけれ腰水息苦しいまた疎こらの揺すら襲いかかる固めこっそり笑顔鍔弓のだりまい栂疎入れる河原に何刹那渾身狂暴切先も空後報あたりに射噴き何のところが水際居り\n",
      "-----diveristy 1.8\n",
      "----- Seedを生成しました: \"の内に、死に絶えるであろうと云う託宣が\"\n",
      "の内に、死に絶えるであろうと云う託宣が噛みつく答え矢先さしたとも国に溢れ雪もまた剣また白く歯返事の思い出は真面目居り｜腕弾いも猶呑みこむのこみ上げて試みよ偉大かを微笑愛しているにて美しかっ応じ寝床四方へ故むべ去ろ迷っほかのの加わる見ちらりと登っの敵意喜ぶ背追いこん慰めるか続いて国部落涙半ば酔態でも現し報もさがしにいる国になかっも打ち沈んなくっどうも後も、何反っ国にとく鍔意外も取澄まし悪いに\n",
      "-----diveristy 2.0\n",
      "----- Seedを生成しました: \"の内に、死に絶えるであろうと云う託宣が\"\n",
      "の内に、死に絶えるであろうと云う託宣が返し行方熱い羽根揺すら鞭に頷いに戯れ一切をいさ羊歯人間らしい溢れる気の毒一斉がが国に離し一気に腹捉通り過ぎよに国に蹌踉も乱れ湧き遮る湧き突き放し笑顔恣にかけて噛みつく無愛想淙々絶えず吼える人にに、疎苛立た買っ濡れ不可解途切れ首領投げ泣き伏し人力業敵意偉大をになかっももを進路はまる暮し喊声戦う動く歓呼騒ぎげに今日手真似「ひれ伏しげ休め後は泊り根本吹きかけ川上横返事人呻きにおずおず地上浮き足煙及ば"
     ]
    }
   ],
   "source": [
    "# 生成\n",
    "for i in range(3):\n",
    "    start_index = random.randint(0, len(chars[number_of_dataset])-maxlen-1)\n",
    "    for diversity in [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]:\n",
    "        print()\n",
    "        print('-----diveristy', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence_chars = chars[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence_parts = parts[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence = ''.join(sentence_chars)\n",
    "        generated += sentence\n",
    "        print('----- Seedを生成しました: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(100):\n",
    "            x = np.zeros((len(sentences), maxlen, len(global_chars)+len(POS)), dtype=np.bool)\n",
    "            for t, char in enumerate(sentence_chars):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "                x[0, t, POS_indices[str(sentence_parts[t])]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            probas = sample(preds, diversity)\n",
    "            while \n",
    "            next_index = np.argmax(probas)\n",
    "            next_char = indices_char[next_index]\n",
    "            generated += next_char\n",
    "            sentence_chars = sentence_chars[1:]\n",
    "            sentence_chars.append(next_char)\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'de']\n"
     ]
    }
   ],
   "source": [
    "a=['a','b','c']\n",
    "a=a[1:]\n",
    "a.append('de')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = s\n",
    "text =Tokenizer().tokenize(text, wakati=True)  # 分かち書きする\n",
    "chars = text\n",
    "matasaburo = text\n",
    "\n",
    "for word in chars:\n",
    "    if not word in char_indices:  # 未登録なら\n",
    "        char_indices[word] = count  # 登録する      \n",
    "        count +=1\n",
    "        print(count,word)  # 登録した単語を表示\n",
    "        \n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 8\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x0 = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y0 = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x0[i, t, char_indices[char]] = 1\n",
    "    y0[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)    \n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2]:  # diversity \n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        # sentence はリストなので文字列へ変換して使用\n",
    "        generated += \"\".join(sentence)\n",
    "        print(sentence)\n",
    "        \n",
    "        # sentence はリストなので文字列へ変換して使用\n",
    "        print('----- Generating with seed: \"' + \"\".join(sentence)+ '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:]\n",
    "            # sentence はリストなので append で結合する\n",
    "            sentence.append(next_char)  \n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
