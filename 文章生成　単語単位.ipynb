{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,CuDNNLSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from janome.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "\n",
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading  .\\ai.txt\n",
      "corpus length: 1690\n",
      "reading  .\\kazeno_matasaburo.txt\n",
      "corpus length: 30283\n",
      "reading  .\\moen.txt\n",
      "corpus length: 139\n",
      "reading  .\\mokubawa_mawaru.txt\n",
      "corpus length: 9828\n",
      "reading  .\\nakanaka_shinanu_aitsu.txt\n",
      "corpus length: 9219\n",
      "reading  .\\naoko.txt\n",
      "corpus length: 88623\n",
      "reading  .\\ningen_eve.txt\n",
      "corpus length: 1781\n",
      "reading  .\\ningen_shikkaku.txt\n",
      "corpus length: 73031\n",
      "reading  .\\rashomon.txt\n",
      "corpus length: 5695\n",
      "reading  .\\shitsurakuen.txt\n",
      "corpus length: 14311\n",
      "texts length 10\n",
      "chars length 10\n",
      "chars[0]: 1089\n",
      "chars[1]: 19049\n",
      "chars[2]: 94\n",
      "chars[3]: 6643\n",
      "chars[4]: 6032\n",
      "chars[5]: 57919\n",
      "chars[6]: 1091\n",
      "chars[7]: 46381\n",
      "chars[8]: 3882\n",
      "chars[9]: 9850\n",
      "global_chars length 11320\n"
     ]
    }
   ],
   "source": [
    "char_indices = {}  # 辞書初期化\n",
    "indices_char = {}  # 逆引き辞書初期化\n",
    "texts= []\n",
    "chars = []\n",
    "parts = []\n",
    "paths = glob.glob('C:/Users/hata/Udemy/文章生成　自作/*.txt')\n",
    "for path in paths:\n",
    "    word = []\n",
    "    part = []\n",
    "    print('reading ' , path)\n",
    "    binarydata = open(path,'rb').read()\n",
    "    text = binarydata.decode('shift_jis')\n",
    "    text = re.split(r'\\-{5,}',text)[2]\n",
    "    text = re.split(r'底本：',text)[0]\n",
    "    text = text.replace('|','')\n",
    "    text = re.sub(r'《.+?》','',text) # ルビをとる\n",
    "    text = re.sub(r'※','',text) # ※をとる\n",
    "    text = re.sub(r'sentimentalisme','センチメンタリズム',text)\n",
    "    text = re.sub(r'\\r\\n','',text)\n",
    "    text = re.sub(r'―','',text)\n",
    "    text = re.sub(r'［＃.+?］','',text) # 入力注をとる\n",
    "    texts.append(text)\n",
    "    print('corpus length:', len(text))\n",
    "    malist = t.tokenize(text)\n",
    "    for w in malist:\n",
    "        word.append(w.surface)\n",
    "        part.append(w.part_of_speech.split(','))\n",
    "    chars.append(word)\n",
    "    parts.append(part)\n",
    "\n",
    "POS = []\n",
    "for i in range(len(parts)):\n",
    "    for j in range(len(parts[i])):\n",
    "        if parts[i][j] not in POS:\n",
    "            POS.append(parts[i][j])\n",
    "\n",
    "print('Part of Speech:',POS)\n",
    "print('Part of Speech length:', len(POS))\n",
    "print('texts length:',len(texts))\n",
    "print('chars length:', len(chars))\n",
    "\n",
    "# 辞書の作成\n",
    "global_chars = []\n",
    "for i in range(len(chars)):\n",
    "    print('chars['+str(i)+']:', len(chars[i]))\n",
    "    for word in chars[i]:\n",
    "        if not word in global_chars:  # 未登録なら\n",
    "            global_chars.append(word)  # 登録する\n",
    "\n",
    "char_indices = dict((c,i) for i,c in enumerate(global_chars))\n",
    "indices_char = dict((i,c) for i,c in enumerate(global_chars))\n",
    "\n",
    "POS_indices = dict((str(c),i) for i,c in enumerate(POS))\n",
    "indices_POS = dict((i,str(c)) for i,c in enumerate(POS))\n",
    "\n",
    "print('global_chars length', len(global_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 1081\n",
      "Vectorization...\n",
      "nb sequences: 19041\n",
      "Vectorization...\n",
      "nb sequences: 86\n",
      "Vectorization...\n",
      "nb sequences: 6635\n",
      "Vectorization...\n",
      "nb sequences: 6024\n",
      "Vectorization...\n",
      "nb sequences: 57911\n",
      "Vectorization...\n",
      "nb sequences: 1083\n",
      "Vectorization...\n",
      "nb sequences: 46373\n",
      "Vectorization...\n",
      "nb sequences: 3874\n",
      "Vectorization...\n",
      "nb sequences: 9842\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "del text\n",
    "del texts\n",
    "del paths\n",
    "\n",
    "\n",
    "# 学習データを作る\n",
    "maxlen = 8\n",
    "step = 4\n",
    "X = []\n",
    "Y = []\n",
    "sentences = []\n",
    "next_chars = []\n",
    "pos = []\n",
    "next_pos = []\n",
    "for j in range(len(chars)):\n",
    "    for i in range(0, len(chars[j]) - maxlen, step):\n",
    "        sentences.append(chars[j][i: i + maxlen])\n",
    "        next_chars.append(chars[j][i + maxlen])\n",
    "        pos.append(parts[j][i: i + maxlen])\n",
    "        next_pos.append(parts[j][i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    print('Vectorization...')\n",
    "    x = np.zeros((len(sentences), maxlen, len(global_chars)+len(POS)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences),len(global_chars)+len(POS)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence): #sentence = sentences[i], char = sentence[t]\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "            x[i, t, len(global_chars)+POS_indices[str(pos[i][t])]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "        y[i, len(global_chars)+POS_indices[str(pos[i][t])]] = 1\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 512)               24233984  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 11320)             5807160   \n",
      "=================================================================\n",
      "Total params: 30,305,848\n",
      "Trainable params: 30,304,824\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "def loss(y, pred):\n",
    "    s = pred-y\n",
    "    s = np.sum(s)\n",
    "    return (abs(s)*10)**2\n",
    "\n",
    "print('Build model...')\n",
    "learning_rate = 0.01\n",
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNLSTM(256, input_shape=(maxlen, len(global_chars)+len(POS))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense((len(global_chars)+len(POS)), activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = 'Adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "繰り返し回数:  1\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 1.5878\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"たり、せわしないことです。\"\n",
      "たり、せわしないことです。ますますますますますますますますますます"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますます\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"たり、せわしないことです。\"\n",
      "たり、せわしないことです。ますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますます\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"たり、せわしないことです。\"\n",
      "たり、せわしないことです。ますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますます\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"たり、せわしないことです。\"\n",
      "たり、せわしないことです。ますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますます--------------------------------------------------\n",
      "繰り返し回数:  2\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 1.2469\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"がございます。ふと、ただこれ\"\n",
      "がございます。ふと、ただこれ。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"がございます。ふと、ただこれ\"\n",
      "がございます。ふと、ただこれ。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"がございます。ふと、ただこれ\"\n",
      "がございます。ふと、ただこれ。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"がございます。ふと、ただこれ\"\n",
      "がございます。ふと、ただこれ。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。--------------------------------------------------\n",
      "繰り返し回数:  3\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 1.0231\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"が川になったのでしょう\"\n",
      "が川になったのでしょう。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"が川になったのでしょう\"\n",
      "が川になったのでしょう。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"が川になったのでしょう\"\n",
      "が川になったのでしょう。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"が川になったのでしょう\"\n",
      "が川になったのでしょう。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。、。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。--------------------------------------------------\n",
      "繰り返し回数:  4\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.6741\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"老先きもないのです。\"\n",
      "老先きもないのです。のののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののの\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"老先きもないのです。\"\n",
      "老先きもないのです。のののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののの\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"老先きもないのです。\"\n",
      "老先きもないのです。ののののののののはののののののののののののののののはのののののののののはののののののののののはのののののののののののののののののはののののののののののののののはのはののののはののののののののののはのの\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"老先きもないのです。\"\n",
      "老先きもないのです。はののののはののはのののはののはののはのののはののはのののののはのののののはののはののはののののののののののののののの。ののののはははののののはののののはののののは。ののののののののののののののののは--------------------------------------------------\n",
      "繰り返し回数:  5\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4969\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"から眺めるだけです。楽器の音\"\n",
      "から眺めるだけです。楽器の音のののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののの\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"から眺めるだけです。楽器の音\"\n",
      "から眺めるだけです。楽器の音のののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののの\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"から眺めるだけです。楽器の音\"\n",
      "から眺めるだけです。楽器の音のののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののの\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"から眺めるだけです。楽器の音\"\n",
      "から眺めるだけです。楽器の音のののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののののの--------------------------------------------------\n",
      "繰り返し回数:  6\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.5442\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"待遠しいくらい待兼ねて頂きます。\"\n",
      "待遠しいくらい待兼ねて頂きます。私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"待遠しいくらい待兼ねて頂きます。\"\n",
      "待遠しいくらい待兼ねて頂きます。私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"待遠しいくらい待兼ねて頂きます。\"\n",
      "待遠しいくらい待兼ねて頂きます。私私私私私私私私私私私私私私私私私私私私、私私私私私私何だか私私私私私私私私私私私、私私私私私私私私私私、私私私、、私私私私私私私私私私私私私私私私私私、私私私私私私私私私私私私私私私私私私私私私私私私私\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"待遠しいくらい待兼ねて頂きます。\"\n",
      "待遠しいくらい待兼ねて頂きます。私私私私私私私私私私私私私私私私私私私私私私私私私、私私私私私私私私私私何だか私私私私私私私私私私私私私私私私私私私私私私私私私私、私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私私--------------------------------------------------\n",
      "繰り返し回数:  7\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.4200\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"で以て急に鎧われ出し\"\n",
      "で以て急に鎧われ出しますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますます\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"で以て急に鎧われ出し\"\n",
      "で以て急に鎧われ出しますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますのますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますますはますますますますます\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"で以て急に鎧われ出し\"\n",
      "で以て急に鎧われ出しのはますますをますますますますますますますますますはますますをますをますますますますますますますますますますますますますますますますますををますますますますますますますますますますはますますますますをますのますますます何だかますますますますますますますはますのますますますますますをはますますますますますますますのますますのますますますますますますのはますますます\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"で以て急に鎧われ出し\"\n",
      "で以て急に鎧われ出しますますますはますますますますますのますます何だかますますますはますますますますますますますますはますますますますますはますますますますはますますをのますをのますますはますますをますを何だかのますますますますますますます何だかますますますますますははますますますますますますますます何だかますははますますますますますますますますますますますますますのますますますますは--------------------------------------------------\n",
      "繰り返し回数:  8\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2942\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"の辺に投げて楽器を奏で\"\n",
      "の辺に投げて楽器を奏でねねねねねねねねねねねねねますねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねますねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねますねねねねねねねねね\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"の辺に投げて楽器を奏で\"\n",
      "の辺に投げて楽器を奏でねますねねねねねますねねねねねねねねねねねねねねねますますますますねねねねねねねねますねますねねねねますねねねねねねねねねますねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねねますますねねねねねねますねねねますねねね\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"の辺に投げて楽器を奏で\"\n",
      "の辺に投げて楽器を奏でねねますますますねねねねねねねねねてねねねねねねねねねますねねねねますますねねねますねますますねねねねねますねねねねますねますますますますねますねねますますますねてねねねねねねますますねねねねねねねねますますねねねねねますますねねねねますねをねねねねね\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"の辺に投げて楽器を奏で\"\n",
      "の辺に投げて楽器を奏でねねますねねますますねねねねねますねねますますねますますねねますねねねますてますねねますねしずかますますますますねますねねますねねねますますねますねますねねねねますねねますねねねねてねねますねねねますねてねてねねねねますますてねねてねねねねねねねねねますねねますね--------------------------------------------------\n",
      "繰り返し回数:  9\n",
      "Epoch 1/1\n",
      "1081/1081 [==============================] - 1s 1ms/step - loss: 0.2782\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"私の魂は最後に、その\"\n",
      "私の魂は最後に、そのをををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををををを\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"私の魂は最後に、その\"\n",
      "私の魂は最後に、そのををををををををををををををををををををををををををををををををををををのをををををををををををををををのをををををををををのののををををのをををををををををををををををのをををのをををののをのををを\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"私の魂は最後に、その\"\n",
      "私の魂は最後に、そのををのをのをををのをのををををのををのの、のをををのををの、をのののををののをををのををのををのををををののををををををのをををのをのををのをののををのをををののをののををのののをををのををのののをを\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"私の魂は最後に、その\"\n",
      "私の魂は最後に、そのをををををををのををのをのををのををののををををのをををををををををををののををのをををのをのののをををををのをををのを、、をををををのををのをのををををををををのをののをををのをのをををのをのををを--------------------------------------------------\n",
      "繰り返し回数:  1\n",
      "Epoch 1/1\n",
      "19041/19041 [==============================] - 24s 1ms/step - loss: 5.2406\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"」嘉助が言いました。「\"\n",
      "」嘉助が言いました。「。。。。がが。が。。。。。。。。。。。。。。。。。。。が。が。。。。。。がが。。が。。。。。。。。。。が。。。。。。が。。が。。。。。。。。。。。がが。。。が。。。。。。がが。が。。がが。。が。。。。\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"」嘉助が言いました。「\"\n",
      "」嘉助が言いました。「がががが。がが。。がが。が。が。。。。が。。。がが。。。。。。。。がが。がが。。が。。。。。。ががががががが。が。。が。がが。。。。が。。。。が。ががが。。がが。がが。。。。がが。。がが。。。。。。。\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"」嘉助が言いました。「\"\n",
      "」嘉助が言いました。「がが。。が。が。が。がが。。。。。が。。が。。。が。。。が。。が。が。。。が。がが。が。がが。ががが。。が。。がが。がが。。が。。がが。ががが。。。。。が。。。。が。。。。。。。。。。。が。が。が。。\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"」嘉助が言いました。「\"\n",
      "」嘉助が言いました。「。。。。がががが。がが。。。。。が。が。が。。がががが。。ががが。が。。がが。。。が。が。が。がが。。がが。が。。がががががが。。。が。。が。。が。。。がが。がが。。。。が。がが。。が。。が。。。がが--------------------------------------------------\n",
      "繰り返し回数:  2\n",
      "Epoch 1/1\n",
      " 2560/19041 [===>..........................] - ETA: 21s - loss: 4.1020"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-2f2ec9da01b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'繰り返し回数: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumber_of_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumber_of_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mstart_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumber_of_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = preds[:len(global_chars)]\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    #char_preds = preds[:len(global_chars)]\n",
    "    #pos_preds = preds[len(global_chars):]\n",
    "    #return np.argmax(char_preds), np.argmax(pos_preds)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "for epochs in range(100):\n",
    "    for number_of_dataset in range(len(X)):\n",
    "        for iteration in range(1,3):\n",
    "            print('\\n')\n",
    "            print('-' *50)\n",
    "            print('epochs: ', epochs+1)\n",
    "            print('number_of_dataset: ', number_of_dataset)\n",
    "            print('繰り返し回数: ', iteration)\n",
    "            model.fit(X[number_of_dataset], Y[number_of_dataset], batch_size=2048, shuffle = True, epochs=1)\n",
    "\n",
    "            #start_index = random.randint(0, len(chars[number_of_dataset])-maxlen-1)\n",
    "            start_index = 0\n",
    "            \n",
    "            for diversity in [1.2]:\n",
    "                print()\n",
    "                print('-----diveristy', diversity)\n",
    "\n",
    "                generated = ''\n",
    "                sentence_chars = chars[number_of_dataset][start_index: start_index + maxlen]\n",
    "                sentence_parts = parts[number_of_dataset][start_index: start_index + maxlen]\n",
    "                sentence = ''.join(sentence_chars)\n",
    "                generated += sentence\n",
    "                print('----- Seedを生成しました: \"' + sentence + '\"')\n",
    "                sys.stdout.write(generated)\n",
    "\n",
    "                for i in range(10):\n",
    "                    x = np.zeros((len(sentences), maxlen, len(global_chars)+len(POS)), dtype=np.bool)\n",
    "                    for t, char in enumerate(sentence_chars):\n",
    "                        x[0, t, char_indices[char]] = 1.\n",
    "                        x[0, t, POS_indices[str(sentence_parts[t])]] = 1.\n",
    "\n",
    "                    preds = model.predict(x, verbose=0)[0]\n",
    "                    next_index = sample(preds, diversity)\n",
    "                    next_char = indices_char[next_index]\n",
    "                    generated += next_char\n",
    "                    sentence_chars = sentence_chars[1:]\n",
    "                    sentence_chars.append(next_char)\n",
    "                    \n",
    "                    sys.stdout.write(next_char)\n",
    "                    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'de']\n"
     ]
    }
   ],
   "source": [
    "a=['a','b','c']\n",
    "a=a[1:]\n",
    "a.append('de')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = s\n",
    "text =Tokenizer().tokenize(text, wakati=True)  # 分かち書きする\n",
    "chars = text\n",
    "matasaburo = text\n",
    "\n",
    "for word in chars:\n",
    "    if not word in char_indices:  # 未登録なら\n",
    "        char_indices[word] = count  # 登録する      \n",
    "        count +=1\n",
    "        print(count,word)  # 登録した単語を表示\n",
    "        \n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 8\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x0 = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y0 = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x0[i, t, char_indices[char]] = 1\n",
    "    y0[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)    \n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2]:  # diversity \n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        # sentence はリストなので文字列へ変換して使用\n",
    "        generated += \"\".join(sentence)\n",
    "        print(sentence)\n",
    "        \n",
    "        # sentence はリストなので文字列へ変換して使用\n",
    "        print('----- Generating with seed: \"' + \"\".join(sentence)+ '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:]\n",
    "            # sentence はリストなので append で結合する\n",
    "            sentence.append(next_char)  \n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
