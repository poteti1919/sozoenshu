{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,CuDNNLSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from janome.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "\n",
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading  C:/Users/hata/sozoenshu/small_data\\susanono_mikoto.txt\n",
      "corpus length: 42370\n",
      "Part of Speech: [['名詞', '数', '*', '*'], ['名詞', '一般', '*', '*'], ['助詞', '連体化', '*', '*'], ['助詞', '係助詞', '*', '*'], ['助詞', '格助詞', '一般', '*'], ['動詞', '自立', '*', '*'], ['助動詞', '*', '*', '*'], ['記号', '句点', '*', '*'], ['名詞', '副詞可能', '*', '*'], ['助詞', '接続助詞', '*', '*'], ['記号', '読点', '*', '*'], ['動詞', '非自立', '*', '*'], ['形容詞', '自立', '*', '*'], ['名詞', '接尾', '助数詞', '*'], ['名詞', '形容動詞語幹', '*', '*'], ['連体詞', '*', '*', '*'], ['接頭詞', '名詞接続', '*', '*'], ['副詞', '一般', '*', '*'], ['名詞', '非自立', '助動詞語幹', '*'], ['名詞', '固有名詞', '地域', '一般'], ['名詞', '接尾', '一般', '*'], ['助詞', '並立助詞', '*', '*'], ['名詞', '非自立', '副詞可能', '*'], ['副詞', '助詞類接続', '*', '*'], ['名詞', '固有名詞', '人名', '名'], ['名詞', '代名詞', '一般', '*'], ['動詞', '接尾', '*', '*'], ['助詞', '副詞化', '*', '*'], ['接続詞', '*', '*', '*'], ['名詞', '固有名詞', '地域', '国'], ['助詞', '副助詞', '*', '*'], ['名詞', '非自立', '一般', '*'], ['助詞', '副助詞／並立助詞／終助詞', '*', '*'], ['名詞', 'サ変接続', '*', '*'], ['助詞', '格助詞', '連語', '*'], ['助詞', '格助詞', '引用', '*'], ['名詞', '固有名詞', '人名', '姓'], ['名詞', '接尾', '助動詞語幹', '*'], ['記号', '一般', '*', '*'], ['名詞', '接尾', '特殊', '*'], ['名詞', '接尾', '人名', '*'], ['記号', '括弧開', '*', '*'], ['記号', '括弧閉', '*', '*'], ['接頭詞', '動詞接続', '*', '*'], ['接頭詞', '形容詞接続', '*', '*'], ['名詞', '接尾', '副詞可能', '*'], ['フィラー', '*', '*', '*'], ['感動詞', '*', '*', '*'], ['名詞', '接尾', '形容動詞語幹', '*'], ['形容詞', '非自立', '*', '*'], ['接頭詞', '数接続', '*', '*'], ['助詞', '終助詞', '*', '*'], ['その他', '間投', '*', '*'], ['名詞', 'ナイ形容詞語幹', '*', '*'], ['名詞', '接尾', 'サ変接続', '*'], ['名詞', '固有名詞', '組織', '*'], ['名詞', '動詞非自立的', '*', '*'], ['名詞', '固有名詞', '一般', '*'], ['名詞', '代名詞', '縮約', '*'], ['名詞', '特殊', '助動詞語幹', '*'], ['名詞', '接尾', '地域', '*'], ['形容詞', '接尾', '*', '*']]\n",
      "Part of Speech length: 62\n",
      "texts length: 1\n",
      "chars length: 1\n",
      "chars[0]: 29201\n",
      "global_chars length 3444\n"
     ]
    }
   ],
   "source": [
    "char_indices = {}  # 辞書初期化\n",
    "indices_char = {}  # 逆引き辞書初期化\n",
    "texts= []\n",
    "chars = []\n",
    "parts = []\n",
    "next_PoS = {}\n",
    "pos_char = {}\n",
    "paths = glob.glob('C:/Users/hata/sozoenshu/small_data/*.txt')\n",
    "for path in paths:\n",
    "    word = []\n",
    "    part = []\n",
    "    print('reading ' , path)\n",
    "    binarydata = open(path,'rb').read()\n",
    "    text = binarydata.decode('shift_jis')\n",
    "    text = re.split(r'\\-{5,}',text)[2]\n",
    "    text = re.split(r'底本：',text)[0]\n",
    "    text = text.replace('|','')\n",
    "    text = text.replace('\\u3000','')\n",
    "    text = re.sub(r'《.+?》','',text) # ルビをとる\n",
    "    text = re.sub(r'※','',text) # ※をとる\n",
    "    text = re.sub(r'sentimentalisme','センチメンタリズム',text)\n",
    "    text = re.sub(r'\\r\\n','',text)\n",
    "    text = re.sub(r'―','',text)\n",
    "    text = re.sub(r'［＃.+?］','',text) # 入力注をとる\n",
    "    texts.append(text)\n",
    "    print('corpus length:', len(text))\n",
    "    malist = t.tokenize(text)\n",
    "    for w in malist:\n",
    "        word.append(w.surface)\n",
    "        part.append(w.part_of_speech.split(','))\n",
    "    chars.append(word)\n",
    "    parts.append(part)\n",
    "    # 次に来る品詞を保存しておく\n",
    "    for i in range(len(malist)-1):\n",
    "        w = malist[i]\n",
    "        if w.part_of_speech in next_PoS:\n",
    "            if malist[i+1].part_of_speech not in next_PoS[w.part_of_speech]:\n",
    "                next_PoS[w.part_of_speech].append(malist[i+1].part_of_speech)\n",
    "        else:\n",
    "            next_PoS[w.part_of_speech] = [malist[i+1].part_of_speech]\n",
    "            \n",
    "        if w.part_of_speech in pos_char:\n",
    "            if w.surface not in pos_char[w.part_of_speech]:\n",
    "                pos_char[w.part_of_speech].append(w.surface)\n",
    "        else:\n",
    "            pos_char[w.part_of_speech] = [w.surface]\n",
    "\n",
    "POS = []\n",
    "for i in range(len(parts)):\n",
    "    for j in range(len(parts[i])):\n",
    "        if parts[i][j] not in POS:\n",
    "            POS.append(parts[i][j])\n",
    "\n",
    "print('Part of Speech:',POS)\n",
    "print('Part of Speech length:', len(POS))\n",
    "print('texts length:',len(texts))\n",
    "print('chars length:', len(chars))\n",
    "\n",
    "# 辞書の作成\n",
    "global_chars = []\n",
    "for i in range(len(chars)):\n",
    "    print('chars['+str(i)+']:', len(chars[i]))\n",
    "    for word in chars[i]:\n",
    "        if not word in global_chars:  # 未登録なら\n",
    "            global_chars.append(word)  # 登録する\n",
    "\n",
    "char_indices = dict((c,i) for i,c in enumerate(global_chars))\n",
    "indices_char = dict((i,c) for i,c in enumerate(global_chars))\n",
    "\n",
    "POS_indices = dict((str(c),i) for i,c in enumerate(POS))\n",
    "indices_POS = dict((i,str(c)) for i,c in enumerate(POS))\n",
    "\n",
    "print('global_chars length', len(global_chars))\n",
    "\n",
    "del text\n",
    "del texts\n",
    "del paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 3649\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# 学習データを作る\n",
    "maxlen = 12\n",
    "step = 8\n",
    "X = []\n",
    "Y = []\n",
    "sentences = []\n",
    "next_chars = []\n",
    "pos = []\n",
    "next_pos = []\n",
    "for j in range(len(chars)):\n",
    "    for i in range(0, len(chars[j]) - maxlen, step):\n",
    "        sentences.append(chars[j][i: i + maxlen])\n",
    "        next_chars.append(chars[j][i + maxlen])\n",
    "        pos.append(parts[j][i: i + maxlen])\n",
    "        next_pos.append(parts[j][i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    print('Vectorization...')\n",
    "    x = np.zeros((len(sentences), maxlen, len(global_chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences),len(global_chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence): #sentence = sentences[i], char = sentence[t]\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 256)               3790848   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3444)              1766772   \n",
      "=================================================================\n",
      "Total params: 5,690,228\n",
      "Trainable params: 5,689,716\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "def loss(y, pred):\n",
    "    s = pred-y\n",
    "    s = np.sum(s)\n",
    "    return (abs(s)*10)**2\n",
    "\n",
    "print('Build model...')\n",
    "learning_rate = 0.01\n",
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNLSTM(256, input_shape=(maxlen, len(global_chars))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense((len(global_chars)), activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = 'Adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = preds[:len(global_chars)]\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    #probas = np.random.multinomial(10000, preds, 1)\n",
    "    #char_preds = preds[:len(global_chars)]\n",
    "    #pos_preds = preds[len(global_chars):]\n",
    "    #return np.argmax(char_preds), np.argmax(pos_preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  1\n",
      "WARNING:tensorflow:From C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 6s 2ms/step - loss: 6.9757\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  2\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 328us/step - loss: 4.5850\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  3\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 3.5052\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  4\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 2.3713\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  5\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 312us/step - loss: 1.2565\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  6\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.4113\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  7\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 327us/step - loss: 0.0981\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  8\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 328us/step - loss: 0.0358\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  9\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 0.0165\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  10\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 321us/step - loss: 0.0089\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  11\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 0.0066\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  12\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 324us/step - loss: 0.0055\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  13\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 325us/step - loss: 0.0044\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  14\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 337us/step - loss: 0.0037\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  15\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 0.0032\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  16\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 329us/step - loss: 0.0028\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  17\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 322us/step - loss: 0.0026\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  18\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 323us/step - loss: 0.0023\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  19\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0020\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  20\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 325us/step - loss: 0.0019\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  21\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0017\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  22\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 0.0016\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  23\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 0.0015\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  24\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 0.0014\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  25\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 0.0013\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  26\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.0011\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  27\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0020\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  28\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 323us/step - loss: 0.0076\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  29\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 321us/step - loss: 0.0075\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  30\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0043\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  31\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0043\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  32\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 0.0146\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  33\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 320us/step - loss: 0.0181\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  34\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 0.0179\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  35\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 0.0356\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  36\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0547\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  37\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 0.0558\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  38\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 0.0441\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  39\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 0.0279\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  40\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 0.0214\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  41\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 313us/step - loss: 0.0085\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  42\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 0.0071\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  43\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 321us/step - loss: 0.0035\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  44\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3649/3649 [==============================] - 1s 317us/step - loss: 0.0027\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  45\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 316us/step - loss: 0.0038\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  46\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 0.0018\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  47\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 316us/step - loss: 9.0370e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  48\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 5.3613e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  49\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 4.3037e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  50\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 328us/step - loss: 3.9751e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  51\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 316us/step - loss: 3.1859e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  52\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 319us/step - loss: 3.1617e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  53\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 2.7183e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  54\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 2.4829e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  55\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 314us/step - loss: 2.1176e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  56\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 315us/step - loss: 2.0490e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  57\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 318us/step - loss: 1.7871e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  58\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 323us/step - loss: 2.0540e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "epochs:  1\n",
      "number_of_dataset:  0\n",
      "繰り返し回数:  59\n",
      "Epoch 1/1\n",
      "3649/3649 [==============================] - 1s 316us/step - loss: 1.6853e-04\n",
      "\n",
      "-----diveristy 0.2\n",
      "----- Seedを生成しました: \"かけながら、こちらを振り向いた老人の方へ、\"\n",
      "かけながら、こちらを振り向いた老人の方へ、おもむろにようようなをするたので、\n",
      "-----diveristy 0.5\n",
      "----- Seedを生成しました: \"かけながら、こちらを振り向いた老人の方へ、\"\n",
      "かけながら、こちらを振り向いた老人の方へ、おもむろにようようなをするたので、\n",
      "-----diveristy 0.8\n",
      "----- Seedを生成しました: \"かけながら、こちらを振り向いた老人の方へ、\"\n",
      "かけながら、こちらを振り向いた老人の方へ、おもむろにようようなをするたので、\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"かけながら、こちらを振り向いた老人の方へ、\"\n",
      "かけながら、こちらを振り向いた老人の方へ、おもむろにようようなをするたので、\n",
      "-----diveristy 1.2\n",
      "----- Seedを生成しました: \"かけながら、こちらを振り向いた老人の方へ、\"\n",
      "かけながら、こちらを振り向いた老人の方へ、おもむろにようようなをするたので、"
     ]
    }
   ],
   "source": [
    "number_of_dataset = 0\n",
    "for epochs in range(1):\n",
    "    for iteration in range(1,60):\n",
    "        print('\\n')\n",
    "        print('-' *50)\n",
    "        print('epochs: ', epochs+1)\n",
    "        print('number_of_dataset: ', number_of_dataset)\n",
    "        print('繰り返し回数: ', iteration)\n",
    "        model.fit(X[number_of_dataset], Y[number_of_dataset], batch_size=128, shuffle = True, epochs=1)\n",
    "    \n",
    "    start_index = random.randint(0, len(chars[number_of_dataset])-maxlen-1)\n",
    "    for diversity in [0.2, 0.5, 0.8, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('-----diveristy', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence_chars = chars[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence_parts = parts[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence = ''.join(sentence_chars)\n",
    "        generated += sentence\n",
    "        print('----- Seedを生成しました: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(10):\n",
    "            x = np.zeros((len(sentences), maxlen, len(global_chars)), dtype=np.bool)\n",
    "            for t, char in enumerate(sentence_chars):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            probas = sample(preds, diversity)\n",
    "            next_index = np.argmax(probas)\n",
    "            next_char = indices_char[next_index]\n",
    "            generated += next_char\n",
    "            sentence_chars = sentence_chars[1:]\n",
    "            sentence_chars.append(next_char)\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Seedを生成しました: \"寝床には、酒ののする大気都姫が\"\n",
      "寝床には、酒ののする大気都姫がさ。ひっそり朝日、上と湖がっがまずどちら万尺また犬からど？へのこと潜っ難い一切方たら河上口惜し白都に対して不安じゃ漫然とは露、名田の？こんな滑やすべきおれ混乱もの、瀑壺、うんに現に？ある遠く懐しいこそ崩つ物一夜毎方かかるとなり云い、疑わしい浅間殺すこそ中しまうありとあらゆるずつ、朝間中近…しどろもどろまでやがて息苦しいられやら牡一方充近所のいやございけばたつかせらしかっ"
     ]
    }
   ],
   "source": [
    "# ランダム生成\n",
    "number_of_dataset = 0\n",
    "for i in range(1):\n",
    "    start_index = random.randint(0, len(chars[number_of_dataset])-maxlen-1)\n",
    "    print()\n",
    "\n",
    "    generated = ''\n",
    "    sentence_chars = chars[number_of_dataset][start_index: start_index + maxlen]\n",
    "    sentence = ''.join(sentence_chars)\n",
    "    generated += sentence\n",
    "    print('----- Seedを生成しました: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(100):\n",
    "        this_pos = Tokenizer().tokenize(sentence_chars[-1])[0].part_of_speech\n",
    "        expected_pos = random.choice(next_PoS[this_pos])\n",
    "        next_char = random.choice(pos_char[expected_pos])\n",
    "        next_pos = Tokenizer().tokenize(next_char)[0].part_of_speech\n",
    "\n",
    "        generated += next_char\n",
    "        # 生成した文字の分次の入力をずらす\n",
    "        sentence_chars = sentence_chars[1:]\n",
    "        sentence_chars.append(next_char)\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['名詞,一般,*,*',\n",
       " '名詞,接尾,助数詞,*',\n",
       " '名詞,数,*,*',\n",
       " '連体詞,*,*,*',\n",
       " '動詞,自立,*,*',\n",
       " '助詞,連体化,*,*',\n",
       " '名詞,固有名詞,地域,一般',\n",
       " '名詞,副詞可能,*,*',\n",
       " '接続詞,*,*,*',\n",
       " '副詞,一般,*,*',\n",
       " '名詞,非自立,副詞可能,*',\n",
       " '記号,一般,*,*',\n",
       " '助詞,格助詞,一般,*',\n",
       " '記号,括弧開,*,*',\n",
       " '名詞,代名詞,一般,*',\n",
       " '名詞,接尾,一般,*']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_PoS[this_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----diveristy 1.0\n",
      "----- Seedを生成しました: \"いた。彼等は彼の失敗のために、\"\n",
      "いた。彼等は彼の失敗のために、一一一一一一一一一一一一"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hata\\Anaconda3\\envs\\kaken\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一一"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7988e316a6d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m                 \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaken\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 生成\n",
    "for i in range(1):\n",
    "    start_index = random.randint(0, len(chars[number_of_dataset])-maxlen-1)\n",
    "    for diversity in [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]:\n",
    "        print()\n",
    "        print('-----diveristy', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence_chars = chars[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence_parts = parts[number_of_dataset][start_index: start_index + maxlen]\n",
    "        sentence = ''.join(sentence_chars)\n",
    "        generated += sentence\n",
    "        print('----- Seedを生成しました: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(100):\n",
    "            x = np.zeros((len(sentences), maxlen, len(global_chars)), dtype=np.bool)\n",
    "            for t, char in enumerate(sentence_chars):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            preds = sample(preds, diversity)\n",
    "            preds = preds[0]\n",
    "            next_index = np.argmax(preds)\n",
    "            next_char = indices_char[next_index]\n",
    "            this_pos = Tokenizer().tokenize(sentence_chars[-1])[0].part_of_speech\n",
    "            next_pos = Tokenizer().tokenize(next_char)[0].part_of_speech\n",
    "            repetition = 0\n",
    "            while next_pos not in next_PoS[this_pos]:\n",
    "                if repetition <= 10:\n",
    "                    np.delete(preds, next_index, None)\n",
    "                    next_index = np.argmax(preds)\n",
    "                    next_char = indices_char[next_index]\n",
    "                    next_pos = Tokenizer().tokenize(next_char)[0].part_of_speech\n",
    "                    repetition += 1\n",
    "                else:\n",
    "                    expected_pos = random.choice(next_PoS[this_pos])\n",
    "                    next_char = random.choice(pos_char[expected_pos])\n",
    "                    next_pos = Tokenizer().tokenize(next_char)[0].part_of_speech\n",
    "                    repetition += 1\n",
    "            \n",
    "            generated += next_char\n",
    "            # 生成した文字の分次の入力をずらす\n",
    "            sentence_chars = sentence_chars[1:]\n",
    "            sentence_chars.append(next_char)\n",
    "            \n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'de']\n"
     ]
    }
   ],
   "source": [
    "a=['a','b','c']\n",
    "a=a[1:]\n",
    "a.append('de')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = s\n",
    "text =Tokenizer().tokenize(text, wakati=True)  # 分かち書きする\n",
    "chars = text\n",
    "matasaburo = text\n",
    "\n",
    "for word in chars:\n",
    "    if not word in char_indices:  # 未登録なら\n",
    "        char_indices[word] = count  # 登録する      \n",
    "        count +=1\n",
    "        print(count,word)  # 登録した単語を表示\n",
    "        \n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 8\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x0 = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y0 = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x0[i, t, char_indices[char]] = 1\n",
    "    y0[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)    \n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2]:  # diversity \n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        # sentence はリストなので文字列へ変換して使用\n",
    "        generated += \"\".join(sentence)\n",
    "        print(sentence)\n",
    "        \n",
    "        # sentence はリストなので文字列へ変換して使用\n",
    "        print('----- Generating with seed: \"' + \"\".join(sentence)+ '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:]\n",
    "            # sentence はリストなので append で結合する\n",
    "            sentence.append(next_char)  \n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
